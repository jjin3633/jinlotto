#!/usr/bin/env python3
"""
SEO ëª¨ë‹ˆí„°ë§ ë„êµ¬
- ì‚¬ì´íŠ¸ë§µ, robots.txt ìƒíƒœ í™•ì¸
- ë©”íƒ€íƒœê·¸ ê²€ì¦
- í˜ì´ì§€ ë¡œë”© ì†ë„ ì²´í¬
- ê¸°ë³¸ SEO ìš”ì†Œ ì ê²€
"""

import requests
import time
from datetime import datetime
from bs4 import BeautifulSoup
import json

class SEOMonitor:
    def __init__(self, base_url="https://stretchinglotto.motiphysio.com/"):
        self.base_url = base_url.rstrip('/')
        self.results = {}
    
    def check_sitemap(self):
        """ì‚¬ì´íŠ¸ë§µ ìƒíƒœ í™•ì¸"""
        try:
            response = requests.get(f"{self.base_url}/sitemap.xml", timeout=10)
            self.results['sitemap'] = {
                'status': response.status_code,
                'accessible': response.status_code == 200,
                'size': len(response.content) if response.status_code == 200 else 0
            }
        except Exception as e:
            self.results['sitemap'] = {'status': 'error', 'error': str(e)}
    
    def check_robots_txt(self):
        """robots.txt ìƒíƒœ í™•ì¸"""
        try:
            response = requests.get(f"{self.base_url}/robots.txt", timeout=10)
            self.results['robots'] = {
                'status': response.status_code,
                'accessible': response.status_code == 200,
                'content': response.text[:200] if response.status_code == 200 else None
            }
        except Exception as e:
            self.results['robots'] = {'status': 'error', 'error': str(e)}
    
    def check_main_page_seo(self):
        """ë©”ì¸ í˜ì´ì§€ SEO ìš”ì†Œ í™•ì¸"""
        try:
            start_time = time.time()
            response = requests.get(self.base_url, timeout=15)
            load_time = time.time() - start_time
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # ë©”íƒ€íƒœê·¸ í™•ì¸
                title = soup.find('title')
                description = soup.find('meta', attrs={'name': 'description'})
                keywords = soup.find('meta', attrs={'name': 'keywords'})
                og_title = soup.find('meta', attrs={'property': 'og:title'})
                og_description = soup.find('meta', attrs={'property': 'og:description'})
                
                # JSON-LD êµ¬ì¡°í™” ë°ì´í„° í™•ì¸
                json_ld = soup.find('script', attrs={'type': 'application/ld+json'})
                
                self.results['main_page'] = {
                    'status': response.status_code,
                    'load_time': round(load_time, 2),
                    'title': title.text if title else None,
                    'title_length': len(title.text) if title else 0,
                    'description': description.get('content') if description else None,
                    'description_length': len(description.get('content')) if description else 0,
                    'keywords': keywords.get('content') if keywords else None,
                    'og_title': og_title.get('content') if og_title else None,
                    'og_description': og_description.get('content') if og_description else None,
                    'has_json_ld': json_ld is not None,
                    'json_ld_content': json_ld.text if json_ld else None
                }
            else:
                self.results['main_page'] = {'status': response.status_code, 'error': 'Page not accessible'}
                
        except Exception as e:
            self.results['main_page'] = {'status': 'error', 'error': str(e)}
    
    def check_rss_feed(self):
        """RSS í”¼ë“œ ìƒíƒœ í™•ì¸"""
        try:
            response = requests.get(f"{self.base_url}/rss.xml", timeout=10)
            self.results['rss'] = {
                'status': response.status_code,
                'accessible': response.status_code == 200,
                'size': len(response.content) if response.status_code == 200 else 0
            }
        except Exception as e:
            self.results['rss'] = {'status': 'error', 'error': str(e)}
    
    def run_full_check(self):
        """ì „ì²´ SEO ì ê²€ ì‹¤í–‰"""
        print(f"ğŸ” SEO ëª¨ë‹ˆí„°ë§ ì‹œì‘: {self.base_url}")
        print(f"â° ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("-" * 50)
        
        # ê° í•­ëª© ì ê²€
        print("ğŸ“„ ì‚¬ì´íŠ¸ë§µ í™•ì¸ ì¤‘...")
        self.check_sitemap()
        
        print("ğŸ¤– robots.txt í™•ì¸ ì¤‘...")
        self.check_robots_txt()
        
        print("ğŸ“° RSS í”¼ë“œ í™•ì¸ ì¤‘...")
        self.check_rss_feed()
        
        print("ğŸ  ë©”ì¸ í˜ì´ì§€ SEO í™•ì¸ ì¤‘...")
        self.check_main_page_seo()
        
        return self.results
    
    def print_report(self):
        """ê²°ê³¼ ë¦¬í¬íŠ¸ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“Š SEO ëª¨ë‹ˆí„°ë§ ê²°ê³¼ ë¦¬í¬íŠ¸")
        print("="*60)
        
        # ì‚¬ì´íŠ¸ë§µ ê²°ê³¼
        sitemap = self.results.get('sitemap', {})
        print(f"ğŸ“„ ì‚¬ì´íŠ¸ë§µ: {'âœ… ì •ìƒ' if sitemap.get('accessible') else 'âŒ ì˜¤ë¥˜'}")
        if sitemap.get('accessible'):
            print(f"   - í¬ê¸°: {sitemap.get('size', 0)} bytes")
        
        # robots.txt ê²°ê³¼
        robots = self.results.get('robots', {})
        print(f"ğŸ¤– robots.txt: {'âœ… ì •ìƒ' if robots.get('accessible') else 'âŒ ì˜¤ë¥˜'}")
        
        # RSS í”¼ë“œ ê²°ê³¼
        rss = self.results.get('rss', {})
        print(f"ğŸ“° RSS í”¼ë“œ: {'âœ… ì •ìƒ' if rss.get('accessible') else 'âŒ ì˜¤ë¥˜'}")
        
        # ë©”ì¸ í˜ì´ì§€ ê²°ê³¼
        main = self.results.get('main_page', {})
        if main.get('status') == 200:
            print(f"ğŸ  ë©”ì¸ í˜ì´ì§€: âœ… ì •ìƒ (ë¡œë”©ì‹œê°„: {main.get('load_time', 0)}ì´ˆ)")
            print(f"   - ì œëª©: {main.get('title', 'N/A')[:50]}...")
            print(f"   - ì œëª© ê¸¸ì´: {main.get('title_length', 0)}ì")
            print(f"   - ì„¤ëª… ê¸¸ì´: {main.get('description_length', 0)}ì")
            print(f"   - Open Graph: {'âœ…' if main.get('og_title') else 'âŒ'}")
            print(f"   - JSON-LD: {'âœ…' if main.get('has_json_ld') else 'âŒ'}")
        else:
            print(f"ğŸ  ë©”ì¸ í˜ì´ì§€: âŒ ì˜¤ë¥˜ (ìƒíƒœ: {main.get('status', 'N/A')})")
        
        # SEO ì ìˆ˜ ê³„ì‚°
        score = 0
        total = 4
        
        if sitemap.get('accessible'): score += 1
        if robots.get('accessible'): score += 1
        if rss.get('accessible'): score += 1
        if main.get('status') == 200: score += 1
        
        print(f"\nğŸ† ì „ì²´ SEO ì ìˆ˜: {score}/{total} ({score/total*100:.0f}%)")
        
        # ê°œì„  ì œì•ˆ
        print("\nğŸ’¡ ê°œì„  ì œì•ˆ:")
        if not sitemap.get('accessible'):
            print("   - ì‚¬ì´íŠ¸ë§µ íŒŒì¼ í™•ì¸ í•„ìš”")
        if not robots.get('accessible'):
            print("   - robots.txt íŒŒì¼ í™•ì¸ í•„ìš”")
        if main.get('load_time', 0) > 3:
            print("   - í˜ì´ì§€ ë¡œë”© ì†ë„ ê°œì„  í•„ìš” (3ì´ˆ ì´ìƒ)")
        if main.get('title_length', 0) > 60:
            print("   - ì œëª© ê¸¸ì´ ë‹¨ì¶• ê¶Œì¥ (60ì ì´í•˜)")
        if main.get('description_length', 0) > 160:
            print("   - ì„¤ëª… ê¸¸ì´ ë‹¨ì¶• ê¶Œì¥ (160ì ì´í•˜)")

def main():
    monitor = SEOMonitor()
    monitor.run_full_check()
    monitor.print_report()
    
    # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"seo_report_{timestamp}.json"
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump({
            'timestamp': datetime.now().isoformat(),
            'results': monitor.results
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ìƒì„¸ ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
