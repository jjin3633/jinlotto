#!/usr/bin/env python3
"""
robots.txt ë””ë²„ê¹… ë„êµ¬
Google ì°¨ë‹¨ ë¬¸ì œ ì§„ë‹¨
"""

import requests
from datetime import datetime

def check_robots_txt():
    """robots.txt ìƒì„¸ í™•ì¸"""
    url = "http://stretchinglotto.motiphysio.com/robots.txt"
    
    print(f"ğŸ¤– robots.txt ë””ë²„ê¹…: {url}")
    print("=" * 60)
    
    try:
        response = requests.get(url, timeout=10)
        
        print(f"âœ… ìƒíƒœ ì½”ë“œ: {response.status_code}")
        print(f"âœ… Content-Type: {response.headers.get('content-type', 'N/A')}")
        print(f"âœ… í¬ê¸°: {len(response.content)} bytes")
        
        if response.status_code == 200:
            print(f"\nğŸ“„ robots.txt ë‚´ìš©:")
            print("-" * 40)
            print(response.text)
            print("-" * 40)
            
            # ë¬¸ì œ ì§„ë‹¨
            content = response.text.lower()
            
            print(f"\nğŸ” ë¬¸ì œ ì§„ë‹¨:")
            
            # 1. Disallow ê·œì¹™ í™•ì¸
            if 'disallow: /' in content and 'allow:' not in content:
                print("âŒ ëª¨ë“  í¬ë¡¤ë§ ì°¨ë‹¨ë¨: 'Disallow: /' ë°œê²¬")
                return False
            elif 'disallow: /sitemap.xml' in content:
                print("âŒ ì‚¬ì´íŠ¸ë§µ ì§ì ‘ ì°¨ë‹¨ë¨")
                return False
            elif 'disallow: /rss.xml' in content:
                print("âŒ RSS ì§ì ‘ ì°¨ë‹¨ë¨")
                return False
            else:
                print("âœ… ì°¨ë‹¨ ê·œì¹™ ì—†ìŒ")
            
            # 2. Allow ê·œì¹™ í™•ì¸
            if 'allow: /' in content:
                print("âœ… ì „ì²´ í—ˆìš© ê·œì¹™ ìˆìŒ")
            else:
                print("âš ï¸ ëª…ì‹œì  í—ˆìš© ê·œì¹™ ì—†ìŒ")
            
            # 3. ì‚¬ì´íŠ¸ë§µ ì„ ì–¸ í™•ì¸
            if 'sitemap:' in content:
                print("âœ… ì‚¬ì´íŠ¸ë§µ ì„ ì–¸ë¨")
            else:
                print("âš ï¸ ì‚¬ì´íŠ¸ë§µ ë¯¸ì„ ì–¸")
            
            return True
        else:
            print(f"âŒ robots.txt ì ‘ê·¼ ì‹¤íŒ¨: {response.status_code}")
            return False
            
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")
        return False

def test_specific_urls():
    """íŠ¹ì • URLë“¤ì˜ robots.txt ì¤€ìˆ˜ ì—¬ë¶€ í™•ì¸"""
    print(f"\nğŸ¯ íŠ¹ì • URL í…ŒìŠ¤íŠ¸")
    print("-" * 40)
    
    test_urls = [
        'http://stretchinglotto.motiphysio.com/',
        'http://stretchinglotto.motiphysio.com/sitemap.xml',
        'http://stretchinglotto.motiphysio.com/rss.xml'
    ]
    
    for url in test_urls:
        print(f"\nğŸ“„ í…ŒìŠ¤íŠ¸: {url}")
        
        # robots.txt ì²´í¬ ì‹œë®¬ë ˆì´ì…˜
        try:
            response = requests.get(url, timeout=10)
            print(f"   âœ… ì ‘ê·¼ ê°€ëŠ¥: {response.status_code}")
        except Exception as e:
            print(f"   âŒ ì ‘ê·¼ ì‹¤íŒ¨: {e}")

def generate_correct_robots():
    """ì˜¬ë°”ë¥¸ robots.txt ìƒì„±"""
    print(f"\nğŸ”§ ì˜¬ë°”ë¥¸ robots.txt ìƒì„±")
    print("-" * 40)
    
    correct_robots = """User-agent: *
Allow: /

# SEO íŒŒì¼ë“¤ ëª…ì‹œì  í—ˆìš©
Allow: /sitemap.xml
Allow: /rss.xml
Allow: /robots.txt

# ì‚¬ì´íŠ¸ë§µ ìœ„ì¹˜ ì„ ì–¸
Sitemap: http://stretchinglotto.motiphysio.com/sitemap.xml
"""
    
    print("ê¶Œì¥ robots.txt ë‚´ìš©:")
    print(correct_robots)
    
    # íŒŒì¼ ì €ì¥
    paths = [
        'frontend/robots.txt',
        'backend/static/robots.txt'
    ]
    
    for path in paths:
        try:
            with open(path, 'w', encoding='utf-8') as f:
                f.write(correct_robots)
            print(f"âœ… {path} ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ {path} ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

def main():
    print("ğŸš¨ robots.txt ì°¨ë‹¨ ë¬¸ì œ í•´ê²° ë„êµ¬")
    print(f"â° ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # robots.txt í™•ì¸
    is_ok = check_robots_txt()
    
    # íŠ¹ì • URL í…ŒìŠ¤íŠ¸
    test_specific_urls()
    
    if not is_ok:
        print("\nğŸ”§ ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì˜¬ë°”ë¥¸ robots.txtë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
        generate_correct_robots()
        
        print(f"\nğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:")
        print("1. ë³€ê²½ì‚¬í•­ ë°°í¬")
        print("2. 10-15ë¶„ í›„ Google Search Consoleì—ì„œ ì¬í…ŒìŠ¤íŠ¸")
        print("3. URL ê²€ì‚¬ ë„êµ¬ë¡œ ë‹¤ì‹œ í™•ì¸")
    
    print(f"\nâœ… robots.txt ì§„ë‹¨ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
